{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e4aaea0-a910-4a3e-8abd-f6e29eb52a1c",
   "metadata": {},
   "source": [
    "# Walkable Accessibility Score for Years 1997 to 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0badfdb7-6f98-42ba-9256-e803dce0c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries -- Takes 1.30 min approx\n",
    "from sklearn.neighbors import BallTree\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from scipy import stats # for correlation\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111d305f-4d35-4645-9b30-5e67dbc7fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only have to read once\n",
    "# Read in the 2015 Block Group Shapefile for all the US.\n",
    "s_v = gpd.read_file('../data/nhgis0022_shape/nhgis0022_shapefile_tl2015_us_blck_grp_2015/US_blck_grp_2015_mainland_geoda.shp') # Load geography (oftentimes as shapefile).\n",
    "\n",
    "# Add 2011 GreatSchools school data\n",
    "sch = gpd.read_file('../data/GreatSchools_2011_us48/GreatSchools_2011_us48.shp') \n",
    "sch = sch.to_crs('esri:102003')\n",
    "\n",
    "#2021 ESRI parks data (centroids)\n",
    "prk = gpd.read_file('../data/Centroids_for_USA_Parks_2021_Buffer2/Centroids_for_USA_Parks_2021_Buffer2.shp') \n",
    "prk = prk.to_crs('esri:102003')\n",
    "\n",
    "# Change the Coordinate Reference System\n",
    "s_v = s_v.set_crs('esri:102003', allow_override=True) # Set the Coordinate Reference System\n",
    "s_v.rename(columns={'GEOID': 'ID'}, inplace=True) # Rename the columns for convenience\n",
    "\n",
    "# Extract the centroids of the polygons.\n",
    "# Replace the column \"geometry\" with the centroids of geography.\n",
    "# This will change the geometry from \"polygon\" to \"point\" geometry.\n",
    "s_v['geometry'] = s_v.centroid\n",
    "\n",
    "s_v_filtered = s_v[['ID', 'geometry']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336fc330-ce51-457d-b2c2-70bd5c9cc6d1",
   "metadata": {},
   "source": [
    "## Create functions to run the walkable accessibility score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b929c6c8-7c48-4311-a364-f9b106b62129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is creating a function for eastimating nearest neighbors from point to point.\n",
    "def get_nearest_neighbors(gdf1, gdf2, k_neighbors=2):\n",
    "    '''Find k nearest neighbors for all source points from a set of candidate points\n",
    "    modified from: https://automating-gis-processes.github.io/site/notebooks/L3/nearest-neighbor-faster.html    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf1 : geopandas.DataFrame\n",
    "    Geometries to search from.\n",
    "    gdf2 : geopandas.DataFrame\n",
    "    Geoemtries to be searched.\n",
    "    k_neighbors : int, optional\n",
    "    Number of nearest neighbors. The default is 2.\n",
    "    Returns\n",
    "    -------\n",
    "    gdf_final : geopandas.DataFrame\n",
    "    gdf1 with distance, index and all other columns from gdf2.'''\n",
    "\n",
    "    src_points = [(x,y) for x,y in zip(gdf1.geometry.x , gdf1.geometry.y)]\n",
    "    candidates =  [(x,y) for x,y in zip(gdf2.geometry.x , gdf2.geometry.y)]\n",
    "\n",
    "    # Create tree from the candidate points\n",
    "    tree = BallTree(candidates, leaf_size=15, metric='euclidean')\n",
    "\n",
    "    # Find closest points and distances\n",
    "    distances, indices = tree.query(src_points, k=k_neighbors)\n",
    "\n",
    "    # Transpose to get distances and indices into arrays\n",
    "    distances = distances.transpose()\n",
    "    indices = indices.transpose()\n",
    "\n",
    "    closest_gdfs = []\n",
    "    for k in np.arange(k_neighbors):\n",
    "        gdf_new = gdf2.iloc[indices[k]].reset_index()\n",
    "        gdf_new['distance'] =  distances[k]\n",
    "        gdf_new = gdf_new.add_suffix(f'_{k+1}')\n",
    "        closest_gdfs.append(gdf_new)\n",
    "    \n",
    "    closest_gdfs.insert(0,gdf1)    \n",
    "    gdf_final = pd.concat(closest_gdfs,axis=1)\n",
    "\n",
    "    return gdf_final\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    # Create the ID2 column\n",
    "    df[\"ID2\"] = df.index\n",
    "\n",
    "    # Reshape the dataframe from wide to long format using the provided suffix\n",
    "    long_df = pd.wide_to_long(df, stubnames=[\"distance_\", \"index_\", \"geometry_\"], i=\"ID2\", j=\"neighbor\")\n",
    "\n",
    "    # Rename columns\n",
    "    long_df.loc[:, 'origin'] = long_df['ID']\n",
    "    long_df.loc[:, 'dest'] = long_df['index_']\n",
    "    long_df.loc[:, 'euclidean'] = long_df['distance_']\n",
    "\n",
    "    # Reset index and keep necessary columns\n",
    "    long_df = long_df.reset_index(level=\"neighbor\")\n",
    "    cost_df = long_df[['euclidean', 'origin', 'dest', 'neighbor']]\n",
    "\n",
    "    # Sort the dataframe by origin and euclidean distance\n",
    "    cost_df.sort_values(by=['origin', 'euclidean'], inplace=True)\n",
    "\n",
    "    return cost_df\n",
    "\n",
    "def access_measure(df_cost, df_sv, upper, decay):\n",
    "    # Calculate time from euclidean distance\n",
    "    # https://journals-sagepub-com.may.idm.oclc.org/doi/10.1177/0265813516641685\n",
    "    df_cost['time'] = (df_cost['euclidean'] * 3600) / 5000  # convert distance into time (rate of 5kph)\n",
    "    \n",
    "    # Calculate LogitT_5 using the provided formula\n",
    "    df_cost['LogitT_5'] = 1 - (1 / (np.exp((upper / 180) - decay * df_cost['time']) + 1))\n",
    "    \n",
    "    # Sum weighted distances by tract (origin) ID\n",
    "    cost_sum = df_cost.groupby(\"origin\").sum()\n",
    "    cost_sum['ID'] = cost_sum.index\n",
    "    \n",
    "    # Merge with the corresponding smaller sv original dataframe\n",
    "    cost_merge = df_sv.merge(cost_sum, how='inner', on='ID')\n",
    "    \n",
    "    return cost_merge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a350761-7dc4-4f9a-827d-28cb024ed941",
   "metadata": {},
   "source": [
    "## Create the function to process the data from years 1997-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad605b68-0c56-4fd8-8b97-7a24eb789194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_business_data(year):\n",
    "    # Load data for the specified year\n",
    "    gz_file_path = f'../data/InfoUSA Data/{year}/{year}_Business_Academic_QCQ.txt.gz'\n",
    "    \n",
    "    # Open the gzipped file and read its content\n",
    "    with gzip.open(gz_file_path, 'rt', encoding='latin-1') as f:\n",
    "        df = pd.read_csv(f, sep=\",\", encoding='latin-1')\n",
    "\n",
    "    # Amenities: groceries, restaurants, coffee shops, banks, parks, schools, bookstores, entertainment, and general shopping establishments \n",
    "    #schools (https://nces.ed.gov/programs/edge/geographic/schoollocations) and parks (centroids - https://www.arcgis.com/home/item.html?id=f092c20803a047cba81fbf1e30eff0b5)\n",
    "\n",
    "    #Convert the column to string\n",
    "    df['Primary NAICS Code'].astype(str)\n",
    "\n",
    "    #Created new categories of NAICS codes so it was easier to filter the categories of interest.\n",
    "    df['NAICS'] = df['Primary NAICS Code'].astype(str)\n",
    "    df['NAICS2'] = df.NAICS.str[:2]\n",
    "    df['NAICS4'] = df.NAICS.str[:4]\n",
    "    df['NAICS6'] = df.NAICS.str[:6]\n",
    "    df.NAICS4.value_counts()\n",
    "\n",
    "    # Filter by specific amenity NAICS codes\n",
    "\n",
    "    filtered = df.loc[(df['NAICS2'] == '72') | (df['NAICS4'] == '4421') | (df['NAICS4'] == '4431') | (df['NAICS4'] == '4451') | \n",
    "                    (df['NAICS4'] == '4461') | (df['NAICS4'] == '4481') | (df['NAICS4'] == '4482') | (df['NAICS4'] == '4483') |\n",
    "                    (df['NAICS4'] == '4511') | (df['NAICS4'] == '4531') | (df['NAICS4'] == '4532') | (df['NAICS4'] == '4539') |\n",
    "                    (df['NAICS4'] == '4453') | (df['NAICS4'] == '4523') | (df['NAICS4'] == '5221') | (df['NAICS6'] == '311811') |\n",
    "                    (df['NAICS6'] == '451211')]\n",
    "\n",
    "    # Remove Puerto Rico, Alaska, Hawaii, and US Virgin Islands because we will be measuring distances and islands will affect our analysis\n",
    "    filtered = filtered[(filtered['State'] != 'PR') & (filtered['State'] != 'AK') & (filtered['State'] != 'HI') & (filtered['State'] != 'VI')]\n",
    "\n",
    "    # Making sure that the latitude and longitude include all decimal points. # Is this right?\n",
    "    filtered = filtered[filtered.Longitude != '-000.000-76']\n",
    "    filtered = filtered[filtered.Latitude != '-000.000-76']\n",
    "\n",
    "    # Create a geodataframe from coordinates (latitude and longitude)\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        filtered,\n",
    "        geometry=gpd.points_from_xy(filtered.Longitude, filtered.Latitude),\n",
    "        crs='epsg:4326') # epsg specifies the projection\n",
    "\n",
    "        # Change the Coordinate Reference System (CRS)\n",
    "    # Check for different projections here: https://epsg.io/\n",
    "    gdf = gdf.to_crs('esri:102003')\n",
    "\n",
    "    # Make sure that the geometry for each row has a value\n",
    "    gdf = gdf[~gdf.is_empty]\n",
    "\n",
    "    lst=[gdf,sch,prk]\n",
    "    am=pd.concat(lst, ignore_index=True, axis=0)\n",
    "    am[\"ID\"] = am.index\n",
    "\n",
    "    #Change this later (Irene)\n",
    "    am_id = gdf[['geometry']]\n",
    "    am_id\n",
    "\n",
    "    #For 50 NN: #10 seconds\n",
    "    closest50 = get_nearest_neighbors(s_v, am_id, k_neighbors=50)\n",
    "    cost50 = clean_dataframe(closest50)\n",
    "    result50_800 = access_measure(cost50, s_v_filtered, upper=800, decay=.008)\n",
    "\n",
    "\n",
    "    result50_800['ID'] = result50_800['ID'].astype(str)\n",
    "    \n",
    "    # Save the final dataframe to a CSV with the year included in the file name\n",
    "    output_file = f'result50_800_{year}.csv'\n",
    "    output_file = f'../output/result50_800_{year}.csv'\n",
    "    result50_800.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61189e13-145b-4782-813b-edea6f85dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(1997, 2020):  # Adjust the range as needed\n",
    "    process_business_data(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bafb2c-a134-4785-bfaf-21d6beda4adc",
   "metadata": {},
   "source": [
    "## Merge CSV files into one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b890cdb5-dd83-483a-9a60-f0e9d297abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge .csv files\n",
    "def merge_csv_files(csv_path_pattern, start_year, end_year, output_file):\n",
    "    # Initialize an empty DataFrame to hold the merged data\n",
    "    merged_data = None\n",
    "\n",
    "    # Loop through each year\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        # Generate the CSV file path for the current year\n",
    "        csv_file = csv_path_pattern % year\n",
    "        \n",
    "        # Read the CSV file\n",
    "        csv_data = pd.read_csv(csv_file, dtype={'ID': str})\n",
    "        \n",
    "        # Select the 'ID' and 'LogitT_5' columns and rename 'LogitT_5' to 'WAS{year}'\n",
    "        csv_data = csv_data[['ID', 'LogitT_5']].rename(columns={'LogitT_5': f'WAS{year}'})\n",
    "        \n",
    "        # Merge the current CSV data with the previously merged data\n",
    "        if merged_data is None:\n",
    "            # For the first year, initialize the merged data\n",
    "            merged_data = csv_data\n",
    "        else:\n",
    "            # Merge subsequent years on the 'ID' column\n",
    "            merged_data = pd.merge(merged_data, csv_data, on='ID', how='outer')\n",
    "    \n",
    "    # Save the merged data to a CSV file\n",
    "    merged_data.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc0af86-177c-476b-8b52-0c5f26dacbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "merge_csv_files(\n",
    "  csv_path_pattern = \"../output/result50_800_%d.csv\",  # %d will be replaced by the year\n",
    "  start_year = 1997,\n",
    "  end_year = 2019,\n",
    "    output_file=\"../output/merged_output.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e00978-3200-400d-904a-084c2048e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.read_csv(\"../output/merged_output.csv\", dtype={'ID': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38500eb-412c-4548-b550-a4fc21a37e4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3b11b9-7d50-4097-9fc2-1ff0355d02da",
   "metadata": {},
   "source": [
    "## Merge CSV with block group shapefile (s_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493ce323-2c57-46ff-b7cb-f74d6132f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_shapefile_data = s_v_filtered.merge(merged_data, on='ID', how='left')\n",
    "\n",
    "#Adding geometry column to the end\n",
    "columns = [col for col in final_shapefile_data.columns if col != 'geometry'] + ['geometry']\n",
    "final_shapefile_data = final_shapefile_data[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48791038-9a2f-4c32-8c81-fdb4834b6518",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_shapefile_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33aa3ee-1c4d-45b1-8e38-b322e46bab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_shapefile_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12bbc47-7d25-493d-81f8-6c9bda0299fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write shapefile to the output folder\n",
    "final_shapefile_data.to_file(\"../output/US_WAS_1997_2019.shp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
